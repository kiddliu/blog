[浮点数的比较，2012版](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)
==

很多年前我写过一篇关于浮点数比较的文章，而这一篇更是经过了深思熟虑，也经过了同行评审，针对浮点数这个复杂的问题给出了扎实的建议和一些令人惊讶的发现。

我们也终于谈到了这个博客系列里我最期待的地方。在这篇文章中我准备分享我的浮点数学知识中最关键的点。那就是：

[浮点]数学太难了。

你完全不会相信它是这样的难。我是说也许你会觉得计算从芝加哥到洛杉矶的火车什么时候翻车比较难，但是跟浮点数学相比那简直是小菜一碟。

我是认真的。每一次当我认为自己掌握了浮点数学中各种细微之处和含义的时候，我都发现我错了——还是有一些令人感到困惑的因素没有考虑到。所以，值得铭记的教训就是浮点数学总是比你想象的更加复杂。希望你在阅读文章剩余部分的时候谨记这一点，并且理解——文章中确实给了一些技巧上的建议，但是它们并不总是有效的。

前情概要
--

这是系列中的第五篇文章。系列的前几篇对于理解这一篇文章非常重要。完整的文章列表如下：

相等的比较
--

浮点数学并不精确。类似0.1这样简单的数并不能用二进制浮点数准确的表示，而浮点数有限的精度意味着诸如运算顺序的细微变化或者中间运算结果的精度都可以影响到结果。这意味着，比较两个浮点数从而判断它们是否相等通常并不是你想要的。GCC甚至对这样的操作给出了警告：“警告：用`==`或者`!=`比较浮点数是不安全的”。

一个体现这种不精确的例子如下：
```cpp
float f = 0.1f;
float sum;
sum = 0;

for (int i = 0; i < 10; ++i)
    sum += f;
float product = f * 10;
printf("sum = %1.15f, mul = %1.15f, mul2 = %1.15f\n",
        sum, product, f * 10);
```
这段代码尝试用三种不同的方式计算数值‘一’：重复的调用加法，和两种稍有不同的乘法。很自然地，我们得到了三个不同的结果，这其中只有一个是1.0：

_sum=1.000000119209290, mul=1.000000000000000, mul2=1.000000014901161_

在这里声明：你获得的结果受你的编译器和编译器设置的影响，实际上这个行为也在帮助我们理解这个问题。

那么，到底发生了什么？哪一个运算结果是正确的？

你所说的“正确”指的是什么？
--

在我们继续这个话题之前，让我们先厘清`0.1`，`float(0.1)`与`double(0.1)`之间的区别。在C/C++中`0.1`与`double(0.1)`是同一个东西，但是在文章里提到0.1的时候我是指那个十进制数字，而`float(0.1)`与`double(0.1)`是去除了尾数的0.1。再澄清一点，`float(0.1)`与`double(0.1)`并没有一样的值，因为`float(0.1)`的二进制表达中数字个数更少，也因此有更大的误差。下边列出了三者的精确数值：

| Number        | Value                                                     |
| ------------- |:----------------------------------------------------------|
| `0.1`           | 0.1                                                     |
| `float(0.1)`  | 0.100000001490116119384765625                             |
| `double(0.1)` | 0.1000000000000000055511151231257827021181583404541015625 |

搞清楚了这些，让我们回过头来看之前代码的输出结果：
1. `sum = 1.000000119209290`：这个计算方法的输入是一个去除了尾数的数值，然后执行了10次加法。其间的每一次加法都有可能去除了尾数，所以有很大的可能产生误差。最终结果不是1.0，也不是`10 * float(0.1)`。然而这个结果是大于1.0的下一个可表示浮点数，所以已经很接近了。
2. `mul = 1.000000000000000`：这个计算方法的输入也是一个去除了尾数的数值，然后执行了一次乘以10，所以没有什么机会引入误差。看起来，从`0.1`到`float(0.1)`的转换向上去除尾数了，但是乘以10碰巧向下去除了尾数，有的时候两次去尾数刚好得到了正确的结果。我们因为错误的原因得到了正确的结果。亦或者这是一个错误的答案，因为它并不是10倍的`float(0.1)`。
3. `mul2 = 1.000000014901161`：这个计算方法的输入也是一个去除了尾数的数值，然后执行了一次双精度乘法，因而避免了之后任何去除尾数的误差。因此我们获得了一个_不一样_的正确答案——精确的`10 * float(0.1)`（结果只能存储在一个双精度浮点数中，而不是一个单精度浮点数中）

总结一下，答案一是错的，但是与正确答案之间只有一个浮点值的差。答案二是正确的（但是并不精确），而答案三是完全正确的（但是看起来是错的）。

接下来呢？
--

现在我们有一些不一样的答案了（我准备直接忽略双精度的答案），那接下来呢？我们是寻找那个等于一的答案，但是如果我们也想把那些足够接近的答案也算进去呢？

精度比较
--

如果比较两个浮点数是否相等是个糟糕的想法，那么检查它们的差是否在某个误差范围或是精度范围之内怎么样？就像这样：

_bool isEqual = fabs(f1 – f2) <= epsilon;_

通过这样的计算我们表达了一个概念，当两个浮点数靠的足够近的时候我们愿意把它们当作相等的。但是我们应该选择什么样的值作为精度呢？

有了上面我们的实验结果，一个很自然的想法就是用这个和里的误差，大概是1.19e<sup>-7</sup>f。事实上在float.h头文件里就有定义这个值，叫做`FLT_EPSILON`。

如此清晰！老天爷发话了，`FLT_EPSILON`就是那个唯一精度值！

然并卵。对于在1.0和2.0之间的数值`FLT_EPSILON`代表着相邻浮点数值差。对于小于1.0的数值来说`FLT_EPSILON`很快就变的太大了，以至于对于足够小的数值来说`FLT_EPSILON`甚至比两个参与比较的数值还要大！

对于大于2.0的数值来说浮点数值之间的差在变大，如果再用`FLT_EPSILON`来比较浮点数的话事倍功半。即如果两个大于2.0的浮点数不相等的话那么它们之间的差一定大于`FLT_EPSILON`。对于大于16777216的数值来说合适的精度实际上已经大于一了，继续使用`FLT_EPSILON`就显得很傻了。我们可不傻。

相对精度比较
--

相对精度比较的想法是得到两个数字的差，然后和数字本身大小做比较。为了得到稳定的结果我们应该总是用差与两个数字中较大的那个数字去比较。用自然语言描述就是：

_为了比较f1与f2，计算diff = fabs(f1 - f2)。如果diff小于n个百分比的max(abs(f1), abs(f2))那么我们称f1与f2相等。_

用代码实现那就是：
```cpp
bool AlmostEqualRelative(float A, float B,
                         float maxRelDiff = FLT_EPSILON)
{
    // Calculate the difference.
    float diff = fabs(A - B);
    A = fabs(A);
    B = fabs(B);
    // Find the largest
    float largest = (B > A) ? B : A;

    if (diff <= largest * maxRelDiff)
        return true;
    return false;
}
```

这个函数还不错。至少大部分时间它工作正常。稍后我会提到它的一些局限性，但首先我想契合这篇文章的核心点——多年前我给出的建议性的技巧。

那就是在做相对精度比较的时候，把`maxRelDiff`设置为`FLT_EPSILON`，或者`FLT_EPSILON`很小的某个倍数。再比它们还小的话就要冒着等价于没有精度值的风险。你当然也可以把它定义的更大一些，如果更大的误差是可以容忍的，但是别大的太过份了。然而为`maxRelDiff`选择一个正确的值需要花精力调整，努力的方向也不明显，而与浮点数格式之间缺乏直接的关联也让我很难过。

ULP
--

我们已经知道相邻的两个浮点数有着相邻的整数表现形式。这意味着如果我们对两个浮点数整数形式相减那么差就告诉我们在浮点空间中两个数字相隔有多远。这给我们带来了：

Dawson的事后诸葛亮原理：

_如果两个同符号浮点数的整数表现形式相减，那么结果的绝对值等于两数之间的浮点数个数再加1。_

也就是说如果两个浮点数相减的结果是一，那么两个数足够接近但是不相等。如果结果是二，那么它们仍然很近，只有一个浮点数在它们之间。那么两个整数形式的差告诉我们它们之间有多少个最小精度差（Units in the Last Place）。通常缩写做ULP，比如“这两个浮点数相差两个ULP”。

让我们尝试一下这个概念：
```cpp
/* See
https://randomascii.wordpress.com/2012/01/11/tricks-with-the-floating-point-format/
for the potential portability problems with the union and bit-fields below.
*/
#include <stdint.h> // For int32_t, etc.

union Float_t
{
    Float_t(float num = 0.0f) : f(num) {}
    // Portable extraction of components.
    bool Negative() const { return i < 0; }
    int32_t RawMantissa() const { return i & ((1 << 23) - 1); }
    int32_t RawExponent() const { return (i >> 23) & 0xFF; }

    int32_t i;
    float f;
#ifdef _DEBUG
    struct
    {   // Bitfields for exploration. Do not use in production code.
        uint32_t mantissa : 23;
        uint32_t exponent : 8;
        uint32_t sign : 1;
    } parts;
#endif
};

bool AlmostEqualUlps(float A, float B, int maxUlpsDiff)
{
    Float_t uA(A);
    Float_t uB(B);

    // Different signs means they do not match.
    if (uA.Negative() != uB.Negative())
    {
        // Check for equality to make sure +0==-0
        if (A == B)
            return true;
        return false;
    }

    // Find the difference in ULPs.
    int ulpsDiff = abs(uA.i - uB.i);
    if (ulpsDiff <= maxUlpsDiff)
        return true;

    return false;
}
```

有意思，也很深奥。

有几个原因导致我们必须检查符号。用二进制补码处理两个有符号数相减并不是特别的有意义，而且相减有可能得到一个33位的结果导致溢出。即使我们处理了这样的情况，这也不会使基于ULP的浮点数比较变得有意义。

_即使一个非常小的数，比如1e<sup>-30</sup>的整数表现形式也有一个非常大的整数部分228737632。所以虽然1e<sup>-30</sup>和0非常近，也和负的1e<sup>-30</sup>非常近但是当单位是ULP的时，这是一个非常大的距离。_

处理了这个特殊情况之后我们只需要简单的减去整数部分，获取绝对值，然后我们知道了它们之间的差是多少。`ulpsDiff`值告诉我们两个浮点数之间有多少个浮点数（加一），这是一种非常直观的与浮点误差打交道的方式。

一个ULP很好（浮点数相邻）。

一百万个ULP很不好（应该不相等）。

用ULP比较数值只是众多相对比较方式中的一种。在极限时它有着完全不一样的特性，但是对于普通的数字来说它很好用。这个概念足够的普通所以boost库有计算两个数字之间ULP的函数。

一个ULP是两个数字之间最小的差。单精度浮点数ULP比双进度浮点数ULP大的多，但是这种命名方式既简洁又方便。我喜欢。

ULP与`FLT_EPSILON`的对决
--

显然，基于ULP比较检查相邻浮点数与使用`AlmostEqualRelative`并把精度设为`FLT_EPSILON`还是很类似的。如果两个参与比较的数字稍微大于二的倍数，那么两者的结果基本一致。而稍微小于二的倍数，使用`FLT_EPSILON`的方法误差是前者的两倍。换个方法举例，如果我们比较4.0与4.0加上两个ULP，那么使用一个ULP作为误差参考与`FLT_EPSILON`相对比较两个方法都会告诉你它们不相等。然而如果我们比较4.0与4.0减去两个ULP，那么前者会告诉你不相等（当然这样是对的）而后者会告诉你它们相等。

这是说得过去的。增加两个ULP给4.0其结果的量级是从4.0减去两个ULP结果量级的两倍，毕竟指数变化了。这两种方法都没有因为这个情况发生变化，只是它们的结果不一致罢了。

如果我的解释还是没让你理解，那也许给你画个图就明白了：

![图](https://randomascii.files.wordpress.com/2012/02/image5.png)

基于ULP的比较也有不同的性能特征。它们极有可能在支持SSE指令集的架构上性能更好，毕竟它更倾向于把浮点数重新解释为整形。然而在其他架构上这种特性也可能会造成非常严重的性能问题，毕竟把浮点数挪到整型寄存器是有开销的。通常这发生在浮点数运算立即接着基于ULP的比较的时候，所以在统计性能的时候要记得考虑这个因素。

通常，仅有一个ULP的差意味着参加比较的两个数字有着相同的量级——一般的两个数值的商不大于1.000000119。但也不总是这样。一些值得注意的例外如下：

* `FLT_MAX`到无穷大——1个ULP，无穷大的比值

* 0到最小的[反常值](https://zh.wikipedia.org/wiki/IEEE_754)——1个ULP，无穷大的比值

* 最小的反常值到下一个反常值——1个ULP，2:1的比值

* NaN——两个NaN可以有类似的甚至一摸一样的表达方式，但是它们不应该用来比较大小

* 正0与负0——二十亿ULP的差，但是它们不应该用来比较大小

* 刚才我们聊到的情况，在二的倍数周围ULP大小不同

例外还是非常的多的。许多情况下你应该忽略NaN（你应该打开非法操作异常从而在发生该异常的时候定位到它们）和无穷（同样原因，使用溢出异常），留下反常值和0们。换句话说，0以及0附近的数字才是大问题。

万恶的0
--

看起来利用相对精度解决问题的这个想法在0附近的时候完全失效了。道理也很简单。如果你期待结果为0，大概可以通过两个数字相减得到。如果想要最精确的0参与计算的数字必须完全一致。如果两个数字相差1个ULP，那么结果相对于两个参与运算的数字很小，但是跟0相比那就太大了。

回想一下文章最一开始的那段代码。我们把`float(0.1)`相加十次于是得到了一个非常接近于1.0的数值，任何一种相对比较方法都是这么判断的。然而如果我们从这个结果里减去1.0那么我们将得到`FLT_EPSILON`，但是我们期望结果是0。如果我们在0与`FLT_EPSILON`之间，或者与任意数之间做相对比较，结果都将是失败。事实上，`FLT_EPSILON`与0之间有872415232个ULP的差，虽然绝大部分人都觉得它足够的小了。

而另外一个例子，考虑一下下边的运算：
```cpp
float someFloat = 67329.234; // 任意选取的
float nextFloat = 67329.242; // 与‘someFloat’刚刚好一个ULP距离
bool equal = AlmostEqualUlps( someFloat, nextFloat, 1); // 返回true, 两个数字只有1个ULP的差
```

我们的实验显示`someFloat`与`nextFloat`距离非常近——它们是邻居。一切都没错。但是考虑一下如果我们让它们两个相减会发生什么：

```cpp
float diff = (nextFloat – someFloat); // 0.0078125000
bool equal = AlmostEqualUlps( diff, 0.0f, 1 ); // 返回false, 距离0有1006632960个ULP的差
```

虽然`someFloat`与`nextFloat`距离非常近，从许多标准来看它们的差非常小，然而`diff`距离0非常非常远，任何基于ULP或者相对比较的测试在与0比较的时候都会失败。

这个问题没有一个简单的答案。然而如果你确信找到了一个简单又通用的答案，你应该重新阅读上边的材料。

最通用的回答既需要用到绝对精度也需要用到相对精度。如果参与比较的两个数字足够的近——不管是哪一种近——都可以把它们当作相等而不用去考虑它们相对的数值。做减法的时候如果你期望得到0这个答案那这个技巧是必要的。而绝对精度的值应该基于相减的两个数值的量级——它应该是一个类似`maxInput * FLT_EPSILON`的数字。然而这也意味着它依赖于算法和输入值。真棒。

同样的，基于ULP的方法在0附近也挂了，原因在定义`AlmostEqualUlps`的时候就讨论过了。

更简单更安全的方法是先检查浮点数的绝对精度，然后视两个不同符号的有符号数不相等。下边是大概的代码，同时适用于相对精度方法和基于ULP的比较方法，绝对精度用来处理数值靠近0的情况：

```cpp
bool AlmostEqualUlpsAndAbs(float A, float B,
            float maxDiff, int maxUlpsDiff)
{
    // Check if the numbers are really close -- needed
    // when comparing numbers near zero.
    float absDiff = fabs(A - B);
    if (absDiff <= maxDiff)
        return true;

    Float_t uA(A);
    Float_t uB(B);

    // Different signs means they do not match.
    if (uA.Negative() != uB.Negative())
        return false;

    // Find the difference in ULPs.
    int ulpsDiff = abs(uA.i - uB.i);
    if (ulpsDiff <= maxUlpsDiff)
        return true;

    return false;
}

bool AlmostEqualRelativeAndAbs(float A, float B,
            float maxDiff, float maxRelDiff = FLT_EPSILON)
{
    // Check if the numbers are really close -- needed
    // when comparing numbers near zero.
    float diff = fabs(A - B);
    if (diff <= maxDiff)
        return true;

    A = fabs(A);
    B = fabs(B);
    float largest = (B > A) ? B : A;

    if (diff <= largest * maxRelDiff)
        return true;
    return false;
}
```

Catastrophic cancellation（灾难性消除，暂译），隐藏在众目睽睽之下
--

如果计算`f1 - f2`然后把结果与0比较，我们知道这是在处理灾难性消除问题，只有在f1与f2相等的时候才会得到0。然而有些时候，减法并不是那么明显的。

考虑下边这段代码：

_sin(pi)_

简单直接。三角函数教给我们它的结果是0。但是你得不到这样的结果。对于双精度pi值和单精度pi值我得到了下边的结果：

_sin(double(pi)) = +0.00000000000000012246467991473532_

_sin(float(pi))     = -0.000000087422776_

如果把它们与0做基于ULP或是相对精度的比较一定会很糟。它们与0之间还有很远的距离。那么到底发生了什么？难道是`sin()`的计算不准确么？

不是的。`sin()`的计算几乎完美——最近的glibc版本完全完美的。甚至Intel的fsin指令，在这个场景下哪怕只给出了很小精度的答案，也在量级上给出了正确的答案。不是`sin()`或者fsin的错——问题出在了别的地方。但是要理解到底发生了什么事情，我们需要。。。微积分！

_对于Intel的fsin指令的深度分析，详见[这篇文章]()_

首先我们要认识到我们并没有让正弦函数计算`sin(pi)`。而是计算了`sin(double(pi))`或者`sin(float(pi))`。然而pi是一个无理数，因而pi不能用一个单精度浮点数准确的表示，一个双精度浮点数也不行。

因而我们真正在计算的是`sin(pi - theta)`，这里theta是一个很小的数，表示pi与`float(pi)`或是`double(pi)`之间的差。

微积分告诉我们（由于sin(pi)的倒数是负数），对于足够小的theta值有`sin(pi - theta) == theta`。因此如果我们使用的正弦函数实现足够精确，我们可以期待`sin(double(pi))`约等于`pi - double(pi)`。也就是说实际上`sin(double(pi))`计算出了`double(pi)`的误差！最好把`sin(float(pi))`表示出来，因为这样我们可以用双精度轻易地的把`float(pi)`加到`sin(float(pi))`：

| Number      | Value                                                     |
|:------------|:----------------------------------------------------------|
| `float(pi)` | +3.1415927410125732                                       |
| `sin(float(pi))`             | -0.0000000874227800                      |
| `float(pi) + sin(float(pi))` | +3.1415926535897966                      |

如果你不记得pi小数点后15位，那么这些数字可能对你来说有点儿无厘头，重点是`float(pi) + sin(float(pi))`比`float(pi)`要更精确地靠近实际pi值。或者说，`sin(float(pi))`告诉你的不是别的，就是`float(pi)`中的误差。

冷静一下，兄弟
--

再强调一遍：`sin(float(pi))`等于`float(pi)`中的误差。

有段时间我觉得这是我发现的最酷的事情，我就是这样的极客。

仔细想一想。因为这太神奇了。下边是sin(‘pi’)与传入的‘pi’值的误差：

```cpp
sin(double(pi)) = +0.0000000000000001224646799147353207
pi-double(pi)   = +0.0000000000000001224646799147353177
sin(float(pi))  = -0.000000087422776
pi-float(pi)    = -0.000000087422780
```

天啊。我们的预测是正确的。作为`double(pi)`的误差，`sin(double(pi))`精确到了小数点后十六到十七位数字，同样的`sin(float(pi))`也精确到小数点后六到七位。`sin(double(pi))`稍欠精确的主要原因是操作符重载把表达式 翻译成了`(float)sin(float(pi))`，只保留了结果的单精度数值。

我觉得我们可以用双精度数学精确的确定双精度常量中的误差是相当棒的。Visual C++ 2015可以打印任意多尾数的`double(pi)`值，而glibc可以精确计算pi()，所以我们用这段代码和手算方法计算得到精确到小数点后30位的pi值：

```cpp
double pi = 3.14159265358979323846;
printf(“%.35f\n%.35f\n”,pi,sin(pi));

3.14159265358979311599796346854418516
0.00000000000000012246467991473532072
```

用计算器就可以立马看到这个有趣的现象。把计算器设置到弧度模式，尝试下边的计算，注意输入值加上结果是如何靠近pi值的。最书呆子的方式：

```cpp
pi = 3.1415926535…

sin(3.14)
= 0.001592652
sin(3.1415)
= 0.000092654
sin(3.141502050)
= 0.000090603
```

核心点是`sin(float(pi))`实际上在计算`pi - float(pi)`，这意味着这是一个经典的灾难性消除。我们预计到0的绝对误差大约有`3.14 * FLT_EPSILON / 2`，而事实上要稍微小一点点。

搞清楚你在做什么
--

对于这个问题没有银弹。你要慎重的选择解决方案。

* 如果你在和0比较，那么相对精度和基于ULP的方案是没有意义的。你需要绝对精度，它的值大约是`FLT_EPSILON`和你的输入值的小几倍。大约吧。

* 如果与非零数值比较，那么相对比较或者基于ULP的比较大概就是你需要的。也许你需要设`FLT_EPSILON`几个倍数的数值为相对精度，或着是很小个数的ULP。如果你确切的知道在与什么样的数值比较，那你可以选定一个绝对精度值。

* 如果你在比较任意两个数你需要把这些都混起来。加油，祝你好运。

毕竟你需要理解你所进行的运算，算法的稳定性如何，当误差太大的时候你应该做些什么。浮点数数学可以异常的精确但是你也需要理解你到底在计算什么。如果你想了解更多关于算法稳定性你应该去读[这篇文章]()并且考虑读一下Michael L Overton写的非常精彩的书。如果你的代码有太大的误差，不要去降低你的精度值而是重构你的代码使它更稳定——有的时候这会导致令人惊讶的结果。

浮点数真实的值
--

为了得到上面的结果我用了为[Fractal eXtreme]()写的无限精确数学库去检查所有高精度的数学计算并打印数值。我也写了一些简单的代码来打印单精度浮点数和双精度浮点数值。下一篇文章我会分享一些更高精度的浮点数值打印方法和代码——从而解开另一个关于浮点数的迷，并且解释为什么浮点数小数部分精度可以有一到一百个数字。

_如今VC++ 2015可以打印单精度浮点数和双精度浮点数的精确值，那么对于Windows开发者来说打印精确的值就不是一件复杂的事情。可以看到精确的数值，例如`double(0.1)`可以帮助我们理解一下复杂的浮点数问题。_
